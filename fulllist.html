<!DOCTYPE html>
<html>

<head>
    <!--    <meta http-equiv="refresh" content="0;url=http://graduatestudents.ucmerced.edu/wlai24/" /> -->
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="kpzhang93.github.io : ">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Shuhuai Ren">
    <link rel="stylesheet" href="./src/main.css">
    <link rel="stylesheet" href="./src/bootstrap.min.css">
    <link rel="stylesheet" href="./src/bootstrap-theme.min.css">

    <link rel="stylesheet" href="http://netdna.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">


    <link rel="stylesheet" href="./src/bootstrap-navbar-custom.css">
    <link rel="stylesheet" href="./src/scrolling-nav.css">
    <link rel="stylesheet" href="./src/mfp.css">

    <link rel="shortcut icon" href="./src/Jamie.jpg">

    <script async="" src="./src/analytics.js"></script>
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', '//www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-45156688-2', 'auto');
        ga('send', 'pageview');
    </script>

    <title>Shuhuai Ren</title>
</head>

<body id="page-top" data-spy="scroll" data-target=".navbar-fixed-top">
<script async defer src="https://buttons.github.io/buttons.js"></script>
<!-- HEADER -->


<section id="publication" class="publication-section">
    <div class="container">
        <h2>Publications (*Equal Contribution)</h2>
        <hr>

        <h3>2025</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/mimo.png">
                    <img src="./papers/mimo.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> MiMo-VL Technical Report</div>
                <div class="author"><b>Shuhuai Ren (Core Contributor)</b>, et al
                </div>

                <div class="conf">Arxiv 2025</div>
                <div class="tag">
                    <span class="label label-default">Arxiv</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2506.03569" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/XiaomiMiMo/MiMo-VL" target="blank">Code&
                Model</a>
            </div>
        </div>
        
        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/tokenbridge.svg">
                    <img src="./papers/tokenbridge.svg" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> TokenBridge: Bridging Continuous and Discrete Tokens for Autoregressive Visual Generation
                </div>
                <div class="author">Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, <b>Shuhuai Ren</b>, Jiashi Feng, Xihui Liu</div>
                <div class="conf">ICCV 2025</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2503.16430"
                                                   target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://yuqingwang1029.github.io/TokenBridge/" target="blank">Code&amp;
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/par.jpg">
                    <img src="./papers/nbp.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Next Block Prediction: Video Generation via Semi-Autoregressive Modeling
                </div>
                <div class="author"><b>Shuhuai Ren</b>, Shuming Ma, Xu Sun, Furu Wei</div>
                <div class="conf">Arxiv 2025</div>
                <div class="tag">
                    <span class="label label-default">Arxiv</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2502.07737"
                                                   target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://renshuhuai-andy.github.io/NBP-project/" target="blank">Code&amp;
                Model</a>
            </div>
        </div>

        <h3>2024</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/ntp_survey.png">
                    <img src="./papers/ntp_survey.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Next Token Prediction Towards Multimodal Intelligence: A Comprehensive Survey
                </div>
                <div class="author">Liang Chen, Zekun Wang, <b>Shuhuai Ren</b>, Lei Li, Haozhe Zhao, Yunshui Li, Zefan Cai, Hongcheng Guo, Lei Zhang, Yizhe Xiong, Yichi Zhang, Ruoyu Wu, Qingxiu Dong, Ge Zhang, Jian Yang, Lingwei Meng, Shujie Hu, Yulong Chen, Junyang Lin, Shuai Bai, Andreas Vlachos, Xu Tan, Minjia Zhang, Wen Xiao, Aaron Yee, Tianyu Liu, Baobao Chang</div>
                <div class="conf">Arxiv 2024</div>
                <div class="tag">
                    <span class="label label-default">Arxiv</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://www.arxiv.org/abs/2412.18619"
                                                   target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/LMM101/Awesome-Multimodal-Next-Token-Prediction" target="blank">Code&amp;
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/par.jpg">
                    <img src="./papers/par.jpg" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Parallelized Autoregressive Visual Generation
                </div>
                <div class="author">Yuqing Wang, <b>Shuhuai Ren</b>, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, Jiashi Feng, Xihui Liu</div>
                <div class="conf">CVPR 2025 (Highlights)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2412.15119"
                                                   target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://epiphqny.github.io/PAR-project/" target="blank">Code&amp;
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/video-mme.png">
                    <img src="./papers/video-mme.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper">Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-modal LLMs in Video Analysis</div>
                <div class="author">Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, <b>Shuhuai Ren</b>, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, Xing Sun
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">CVPR 2025 (Highlights)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2405.21075" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/BradyFU/Video-MME" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/LaDiC.png">
                    <img src="./papers/LaDiC.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> LaDiC: Are Diffusion Models Really Inferior to Autoregressive Counterparts for Image-to-Text Generation?</div>
                <div class="author">Yuchi Wang*, <b>Shuhuai Ren</b>*, Rundong Gao, Linli Yao, Qingyan Guo, Kaikai An, Jianhong Bai, Xu Sun
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">NAACL 2024</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2404.10763" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/wangyuchi369/LaDiC" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/VITATECS.png">
                    <img src="./papers/VITATECS.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models</div>
                <div class="author">Shicheng Li, Lei Li, <b>Shuhuai Ren</b>, Yuanxin Liu, Yi Liu, Rundong Gao, Xu Sun, Lu Hou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">ECCV 2024</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2311.17404" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lscpku/VITATECS" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/TempCompass.png">
                    <img src="./papers/TempCompass.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> TempCompass: Do Video LLMs Really Understand Videos?</div>
                <div class="author">Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, <b>Shuhuai Ren</b>, Lei Li, Sishuo Chen, Xu Sun, Lu Hou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Findings of ACL 2024 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2403.00476" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/llyx97/TempCompass" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/pca-bench.png">
                    <img src="./papers/pca-bench.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> PCA-Bench: Evaluating Multimodal Large Language Models in Perception-Cognition-Action Chain</div>
                <div class="author">Liang Chen, Yichi Zhang, <b>Shuhuai Ren</b>, Haozhe Zhao, Zefan Cai, Yuchi Wang, Peiyi Wang, Xiangdi Meng, Tianyu Liu, Baobao Chang
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Findings of ACL 2024 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2402.15527" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/pkunlp-icler/PCA-EVAL" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/MR-VPC.png">
                    <img src="./papers/MR-VPC.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Towards Multimodal Video Paragraph Captioning Models Robust to Missing Modality</div>
                <div class="author">Sishuo Chen, Lei Li, <b>Shuhuai Ren</b>, Rundong Gao, Yuanxin Liu, Xiaohan Bi, Xu Sun, Lu Hou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Arxiv 2024</div>
                <div class="tag">
                    <span class="label label-default">Arxiv</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2403.19221" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lancopku/MR-VPC" target="blank">Code&
                Model</a>
            </div>
        </div>

        <h3>2023</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/timechat.png">
                    <img src="./papers/timechat.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding</div>
                <div class="author"><b>Shuhuai Ren</b>*, Linli Yao*, Shicheng Li, Xu Sun, Lu Hou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">CVPR 2024</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2312.02051" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/RenShuhuai-Andy/TimeChat" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/testa.png">
                    <img src="./papers/testa.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> TESTA: Temporal-Spatial Token Aggregation for Long-form Video-Language Understanding</div>
                <div class="author"><b>Shuhuai Ren</b>, Sishuo Chen, Shicheng Li, Xu Sun, Lu Hou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Findings of EMNLP 2023 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2310.19060" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/RenShuhuai-Andy/TESTA" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/pomp.png">
                    <img src="./papers/pomp.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Prompt Pre-Training with Twenty-Thousand Classes for Open-Vocabulary Visual Recognition</div>
                <div class="author"><b>Shuhuai Ren</b>, Aston Zhang, Yi Zhu, Shuai Zhang, Shuai Zheng, Mu Li, Alex Smola, Xu Sun
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">NeurIPS 2023</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2304.04704" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/amazon-science/prompt-pretraining" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/fetv.png">
                    <img src="./papers/fetv.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation</div>
                <div class="author">Yuanxin Liu, Lei Li, <b>Shuhuai Ren</b>, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, Lu Hou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">NeurIPS 2023 (Dataset & Benchmark Track)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2311.01813" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/llyx97/FETV" target="blank">Code&
                Model</a>
            </div>
        </div>

<!--        <div class="spacer"></div>-->
<!--        <div class="row item">-->
<!--            <div class="col-md-4 col-sm-6 col-xs-12 item-img">-->
<!--                <a class="image-popup-no-margins" href="./papers/pca_eval.png">-->
<!--                    <img src="./papers/pca_eval.png" class="img-thumbnail img-responsive">-->
<!--                </a>-->
<!--            </div>-->
<!--            <div class="col-md-8 col-sm-6 col-xs-12 item-info">-->
<!--                <div class="paper"> Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond</div>-->
<!--                <div class="author">Liang Chen, Yichi Zhang, <b>Shuhuai Ren</b>, Haozhe Zhao, Zefan Cai, Yuchi Wang, Tianyu Liu, Baobao Chang-->
<!--                </div>-->

<!--                &lt;!&ndash; (* indicates equal contribution) &ndash;&gt;-->
<!--                <div class="conf">NeurIPS 2023 Foundation Models for Decision Making Workshop</div>-->
<!--                <div class="tag">-->
<!--                    <span class="label label-default">Workshop</span>-->
<!--                </div>-->
<!--                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2310.02071" target="blank">Paper</a>-->
<!--                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/pkunlp-icler/PCA-EVAL" target="blank">Code&-->
<!--                Model</a>-->
<!--            </div>-->
<!--        </div>-->

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/m3it.png">
                    <img src="./papers/m3it.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning</div>
                <div class="author">Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, <b>Shuhuai Ren</b>, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Arxiv 2023</div>
                <div class="tag">
                    <span class="label label-default">Arxiv</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2306.04387" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://huggingface.co/papers/2306.04387" target="blank">Dataset</a>
            </div>
        </div>

        <h3>2022</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/openness.png">
                    <img src="./papers/openness.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Delving into the Openness of CLIP</div>
                <div class="author"><b>Shuhuai Ren</b>, Lei Li, Xuancheng Ren, Guangxiang Zhao, Xu Sun
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Findings of ACL 2023 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2206.01986" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lancopku/clip-openness" target="blank">Code&
                Model</a>
            </div>
        </div>

        <h3>2021</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/cuge.png">
                    <img src="./papers/cuge.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper">CUGE: A Chinese Language Understanding and Generation Evaluation Benchmark</div>
<!--                <div class="author">Yuan Yao, ..., <b>Shuhuai Ren</b>, ..., Zhiyuan Liu∗, Xianpei Han∗, Erhong Yang∗, Zhifang Sui∗, Maosong Sun∗-->
                <div class="author">Yuan Yao, Qingxiu Dong, Jian Guan, Boxi Cao, Zhengyan Zhang, Chaojun Xiao, Xiaozhi Wang, Fanchao Qi, Junwei Bao, Jinran Nie, Zheni Zeng, Yuxian Gu, Kun Zhou, Xuancheng Huang, Wenhao Li, <b>Shuhuai Ren</b>, Jinliang Lu, Chengqiang Xu, Huadong Wang, Guoyang Zeng, Zile Zhou, Jiajun Zhang, Juanzi Li, Minlie Huang, Rui Yan, Xiaodong He, Xiaojun Wan, Xin Zhao, Xu Sun, Yang Liu, Zhiyuan Liu∗, Xianpei Han∗, Erhong Yang∗, Zhifang Sui∗, Maosong Sun∗
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="tag">
                    <span class="label label-default">Preprint</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2112.13610" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="http://cuge.baai.ac.cn/#/" target="blank">Benchmark</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/taa.png">
                    <img src="./papers/taa.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification</div>
                <div class="author"><b>Shuhuai Ren</b>, Jinchao Zhang, Lei li, Xu Sun*, Jie Zhou
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">EMNLP 2021 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2109.00523" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lancopku/text-autoaugment" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/dkd.png">
                    <img src="./papers/dkd.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Dynamic Knowledge Distillation for Pre-trained Language Models</div>
                <div class="author">Lei Li, Yankai Lin, <b>Shuhuai Ren</b>, Peng Li, Jie Zhou, Xu Sun*
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">EMNLP 2021 (Long Paper, <span style="color:#FF0000;font-weight:bold;">Oral</span>)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2109.11295" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lancopku/DynamicKD" target="blank">Code&
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/cascadebert.png">
                    <img src="./papers/cascadebert.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade </div>
                <div class="author">Lei Li, Yankai Lin, Deli Chen, <b>Shuhuai Ren</b>, Peng Li, Jie Zhou, Xu Sun*
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">Findings of EMNLP 2021 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2012.14682" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lancopku/CascadeBERT" target="blank">Code&amp;
                Model</a>
            </div>
        </div>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/iais_case.png">
                    <img src="./papers/iais_case.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Learning Relation Alignment for Calibrated Cross-modal Retrieval</div>
                <div class="author"><b>Shuhuai Ren</b>, Junyang Lin, Guangxiang Zhao, Rui Men, An Yang, Jingren Zhou, Xu Sun*, Hongxia
                    Yang
                </div>

                <!-- (* indicates equal contribution) -->
                <div class="conf">ACL 2021 (Long Paper, <span style="color:#FF0000;font-weight:bold;">Oral</span>)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/2105.13868" target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/lancopku/IAIS" target="blank">Code&amp;
                Model</a>
            </div>
        </div>

        <h3>2020</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/dca.png">
                    <img src="./papers/dca.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper">DCA: Diversified Co-Attention towards Informative Live Video Commenting</div>
                <div class="author">Zhihan Zhang, Zhiyi Yin, <b>Shuhuai Ren</b>, Xinhang Li, Shicheng Li</div>
                <div class="conf">NLPCC 2020 (Long Paper)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://arxiv.org/abs/1911.02739" target="blank">Paper</a>
            </div>
        </div>

        <h3>2019</h3>
        <hr>

        <div class="spacer"></div>
        <div class="row item">
            <div class="col-md-4 col-sm-6 col-xs-12 item-img">
                <a class="image-popup-no-margins" href="./papers/pwws.png">
                    <img src="./papers/pwws.png" class="img-thumbnail img-responsive">
                </a>
            </div>
            <div class="col-md-8 col-sm-6 col-xs-12 item-info">
                <div class="paper"> Generating Natural Language Adversarial Examples through Probability Weighted Word
                    Saliency
                </div>
                <div class="author"><b>Shuhuai Ren</b>, Yihe Deng, Kun He*, Wangxiang Che</div>
                <div class="conf">ACL 2019 (Long Paper, <span style="color:#FF0000;font-weight:bold;">Oral</span>)</div>
                <div class="tag">
                    <span class="label label-success">Conference</span>
                </div>
                <i class="fa fa-book fa-fw"></i><a href="https://www.aclweb.org/anthology/P19-1103"
                                                   target="blank">Paper</a>
                <i class="fa fa fa-file-text fa-fw"></i><a href="https://github.com/JHL-HUST/PWWS" target="blank">Code&amp;
                Model</a>
            </div>
        </div>
        <br>
        <br>

    </div> <!-- end of Publication -->

</section>


</div></section>


<div class="footer">
    <div class="container">
        <div class="row item">
            <p align="center"><a href='https://clustrmaps.com/site/1bjja' title='Visit tracker'><img
                    src='//clustrmaps.com/map_v2.png?cl=ffffff&w=a&t=tt&d=PgzBKvDwV0_11OsfeUcddl13pJ1ccAn3Or6O6qYWHuM&co=2d78ad&ct=ffffff'/></a>
            </p>
            <div>Copyright © Shuhuai Ren 2021</div>
            <div>Template from <a href="http://phoenix104104.github.io">Jason Lai</a></div>
            <div class="last-update">
                Last Updated:
                <script language="JavaScript">
                    document.write(document.lastModified);
                </script>
            </div>
            <!--            <div align="center"><a href="http://www.amazingcounters.com"><img src="http://cc.amazingcounters.com/counter.php?i=3246736&c=9740521"-->
            <!--                                                                              alt="AmazingCounters.com"></a></div>-->
        </div>

    </div>

    <script type="text/javascript" src="./src/jquery-1.11.0.js"></script>
    <script src="./src/bootstrap.min.js"></script>
    <!--
    <script src="//code.jquery.com/jquery-1.11.3.min.js"></script>
    <script src="//code.jquery.com/jquery-migrate-1.2.1.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    -->
    <!-- Scrolling Nav JavaScript -->
    <script src="./src/jquery.easing.min.js"></script>
    <script src="./src/scrolling-nav.js"></script>
    <script src="./src/mfp.js"></script>
    <script type="text/javascript" src="./src/main.js"></script>


</div>

</body>
</html>
